{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 \n",
    "\n",
    "#### How to Train your model\n",
    "\n",
    "<img src=\"https://snag.gy/GxVakf.jpg\" width=\"600px\"/>\n",
    "\n",
    "#### Recap from last Lesson\n",
    "\n",
    "- We looked at Conv2D, and looked at how it initializes parameters\n",
    "- We found `math.sqrt(5)` and didn't know the reasoning\n",
    "- How I researched: [fastai nb link](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/02a_why_sqrt5.ipynb)\n",
    "- Step 1: loaded everything we had from last week\n",
    "- Step 2: used MNIST\n",
    "- Step 3: setup a Conv2D layer\n",
    "- Step 4: looked at the first 100 samples\n",
    "- Step 5: look at the mean / std of different weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_lib import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): \n",
    "    return (x-m)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why SQRT(5)?\n",
    "\n",
    "`torch.nn.modules.conv._ConvNd.reset_parameters??`\n",
    "\n",
    "    Signature: torch.nn.modules.conv._ConvNd.reset_parameters(self)\n",
    "    Docstring: <no docstring>\n",
    "    Source:   \n",
    "        def reset_parameters(self):\n",
    "            n = self.in_channels\n",
    "            init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            if self.bias is not None:\n",
    "                fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "    File:      ~/envs/py3/lib/python3.6/site-packages/torch/nn/modules/conv.py\n",
    "    Type:      function\n",
    "    \n",
    "    \n",
    "To test out and understand the reasoning behind the square root 5, we will start with MNIST data and observe how the initialization affects the performance. The following code block will load and prepare the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, tensor(10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our data from MNIST\n",
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "\n",
    "# find our mean and std\n",
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "\n",
    "# normalize the training and the validation data\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_valid = normalize(x_valid, train_mean, train_std)\n",
    "\n",
    "# because the data comes in single vectors, we\n",
    "# will reshape to the 28 x 28 sized 1 channel format images\n",
    "x_train = x_train.view(-1,1,28,28)\n",
    "x_valid = x_valid.view(-1,1,28,28)\n",
    "x_train.shape, x_valid.shape\n",
    "\n",
    "# get the number of samples\n",
    "n, *_ = x_train.shape\n",
    "\n",
    "# number of classes\n",
    "c = y_train.max()+1\n",
    "\n",
    "# number of hidden units\n",
    "nh = 32\n",
    "n, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's experiment with the Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5)\n",
    "\n",
    "# look at the first 100 examples\n",
    "x = x_valid[:100]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 5, 5])\n",
      "(tensor(0.0005, grad_fn=<MeanBackward1>), tensor(0.1133, grad_fn=<StdBackward0>)) (tensor(-0.0158, grad_fn=<MeanBackward1>), tensor(0.1103, grad_fn=<StdBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# we will be keeping track of the weights as the layer trains\n",
    "def stats(x): \n",
    "    \"\"\" Returns mean and std \"\"\"\n",
    "    return x.mean(), x.std()\n",
    "\n",
    "print(l1.weight.shape)\n",
    "print(stats(l1.weight), stats(l1.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the first 100 samples through the Conv2D\n",
    "\n",
    "if we look at the stats of result `t`, we see that the mean is close to zero, but the standard deviation is NOT close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0151, grad_fn=<MeanBackward1>),\n",
       " tensor(0.6587, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = l1(x)\n",
    "stats(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, the regular kaiming normal gets close to mean=0, std=1. The weights resemble a leaky relu layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0083, grad_fn=<MeanBackward1>),\n",
       " tensor(0.9999, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal(l1.weight, a=1.)\n",
    "stats(l1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our own leaky relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def f1(x, a=0):\n",
    "    return F.leaky_relu(l1(x), a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied against the kaiming_normal we get a variance of 1. The mean is no longer 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.5679, grad_fn=<MeanBackward1>),\n",
       " tensor(1.0585, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal(l1.weight, a=0.)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we apply this against the default pytorch value, we don't get mean 0, std 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2036, grad_fn=<MeanBackward1>),\n",
       " tensor(0.3707, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand, we will write our own Kaiming Init function\n",
    "\n",
    "Note that in this case, since we are looking at the convolutions, there's an additional dimension of multiplication\n",
    "\n",
    "Remember from before:\n",
    "\n",
    "```python\n",
    "print(l1.weight.shape)\n",
    "-> torch.Size([32, 1, 5, 5])\n",
    "```\n",
    "We need to multiply the kernel size `[5x5]` times the number of filters `32`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# receptive field size (5 x 5)\n",
    "rec_fs = l1.weight[0, 0].numel()\n",
    "rec_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of filters out, number of filters in\n",
    "nf, ni, *_ = l1.weight.shape\n",
    "nf, ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 800)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fan_in = ni * rec_fs\n",
    "fan_out = nf * rec_fs\n",
    "fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : \t1.0\n",
      "0 : \t1.4142135623730951\n",
      "0.01 : \t1.4141428569978354\n",
      "0.1 : \t1.4071950894605838\n",
      "2.24 : \t0.5773502691896257\n"
     ]
    }
   ],
   "source": [
    "def gain(a):\n",
    "    return math.sqrt(2.0 / (1 + a **2))\n",
    "\n",
    "for a in [1, 0, 0.01, 0.1, math.sqrt(5)]:\n",
    "    print(f\"{np.round(a,2)} : \\t{gain(a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
