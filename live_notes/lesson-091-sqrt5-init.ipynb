{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 \n",
    "\n",
    "#### How to Train your model\n",
    "\n",
    "<img src=\"https://snag.gy/GxVakf.jpg\" width=\"600px\"/>\n",
    "\n",
    "#### Recap from last Lesson\n",
    "\n",
    "- We looked at Conv2D, and looked at how it initializes parameters\n",
    "- We found `math.sqrt(5)` and didn't know the reasoning\n",
    "- How I researched: [fastai nb link](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/02a_why_sqrt5.ipynb)\n",
    "- Step 1: loaded everything we had from last week\n",
    "- Step 2: used MNIST\n",
    "- Step 3: setup a Conv2D layer\n",
    "- Step 4: looked at the first 100 samples\n",
    "- Step 5: look at the mean / std of different weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_lib import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): \n",
    "    return (x-m)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why SQRT(5)?\n",
    "\n",
    "`torch.nn.modules.conv._ConvNd.reset_parameters??`\n",
    "\n",
    "    Signature: torch.nn.modules.conv._ConvNd.reset_parameters(self)\n",
    "    Docstring: <no docstring>\n",
    "    Source:   \n",
    "        def reset_parameters(self):\n",
    "            n = self.in_channels\n",
    "            init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            if self.bias is not None:\n",
    "                fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "    File:      ~/envs/py3/lib/python3.6/site-packages/torch/nn/modules/conv.py\n",
    "    Type:      function\n",
    "    \n",
    "    \n",
    "To test out and understand the reasoning behind the square root 5, we will start with MNIST data and observe how the initialization affects the performance. The following code block will load and prepare the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, tensor(10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our data from MNIST\n",
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "\n",
    "# find our mean and std\n",
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "\n",
    "# normalize the training and the validation data\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_valid = normalize(x_valid, train_mean, train_std)\n",
    "\n",
    "# because the data comes in single vectors, we\n",
    "# will reshape to the 28 x 28 sized 1 channel format images\n",
    "x_train = x_train.view(-1,1,28,28)\n",
    "x_valid = x_valid.view(-1,1,28,28)\n",
    "x_train.shape, x_valid.shape\n",
    "\n",
    "# get the number of samples\n",
    "n, *_ = x_train.shape\n",
    "\n",
    "# number of classes\n",
    "c = y_train.max()+1\n",
    "\n",
    "# number of hidden units\n",
    "nh = 32\n",
    "n, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's experiment with the Conv2d\n",
    "\n",
    "We will make a single conv2d layer, with 1 channel, and a 5x5 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5)\n",
    "\n",
    "# look at the first 100 examples for testing\n",
    "x = x_valid[:100]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the weights, we have\n",
    "- 32 different filters\n",
    "- 1 channel\n",
    "- 5 kernel width\n",
    "- 5 kernel height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 5, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the weights which are initialized with kaiming_normal and with sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0084, grad_fn=<MeanBackward1>), tensor(0.1142, grad_fn=<StdBackward0>)) (tensor(-0.0250, grad_fn=<MeanBackward1>), tensor(0.1251, grad_fn=<StdBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# we will be keeping track of the weights as the layer trains\n",
    "def stats(x): \n",
    "    \"\"\" Returns mean and std \"\"\"\n",
    "    return x.mean(), x.std()\n",
    "\n",
    "print(stats(l1.weight), stats(l1.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the first 100 samples through the Conv2D\n",
    "\n",
    "if we look at the stats of result `t`, we see that the mean is close to zero, but the **standard deviation is NOT close to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0039, grad_fn=<MeanBackward1>),\n",
       " tensor(0.5825, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = l1(x)\n",
    "stats(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, the regular kaiming normal gets close to mean=0, std=1. The weights resemble a leaky relu layer. A leaky relu layer has gradient <0 is called a, or leak. So we don't have anything like that going on (no leak) we have a slope of 1 so `a=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0538, grad_fn=<MeanBackward1>),\n",
       " tensor(1.1656, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal(l1.weight, a=1.)\n",
    "stats(l1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make our own leaky relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def f1(x, leak_amt=0):\n",
    "    return F.leaky_relu(l1(x), leak_amt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied against the kaiming_normal we get a variance of 1. **The mean is no longer 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4494, grad_fn=<MeanBackward1>),\n",
       " tensor(0.7478, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal(l1.weight, a=0.)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we apply our relu against the default pytorch value, **we don't get mean 0, std 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2404, grad_fn=<MeanBackward1>),\n",
       " tensor(0.4425, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand, we will write our own Kaiming Init function\n",
    "\n",
    "Recall: when we do a matrix multiplication of convolution, its not only the input vs. a 2D array, there is an additional dimension that represents **channels**. So its really like multiplying against a volume\n",
    "\n",
    "Remember from before:\n",
    "\n",
    "```python\n",
    "print(l1.weight.shape)\n",
    "-> torch.Size([32, 1, 5, 5])\n",
    "```\n",
    "We need to multiply the kernel size `[5x5]` times the number of filters `32`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# receptive field size (5 x 5)\n",
    "# how many elements in that kernel (only single channel)\n",
    "rec_fs = l1.weight[0, 0].numel()\n",
    "rec_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of filters out, number of filters in\n",
    "nf, ni, *_ = l1.weight.shape\n",
    "nf, ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 800)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input filters times receptive field size\n",
    "fan_in = ni * rec_fs\n",
    "\n",
    "# output filters times receptive field size\n",
    "fan_out = nf * rec_fs\n",
    "fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : \t1.0\n",
      "0 : \t1.4142135623730951\n",
      "0.01 : \t1.4141428569978354\n",
      "0.1 : \t1.4071950894605838\n",
      "2.24 : \t0.5773502691896257\n"
     ]
    }
   ],
   "source": [
    "# for a leaky relu\n",
    "# kaiming init method\n",
    "def gain(leaky_amt):\n",
    "    return math.sqrt(2.0 / (1 + leaky_amt **2))\n",
    "\n",
    "for leaky_amt in [1, 0, 0.01, 0.1, math.sqrt(5)]:\n",
    "    print(f\"{np.round(leaky_amt,2)} : \\t{gain(leaky_amt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the heck is 0.577?\n",
    "\n",
    "Remember what they use to init isnt `kaiming_normal` its actually **`kaiming_uniform`**. Whats the difference?\n",
    "\n",
    "<img src=\"https://snag.gy/xVbW92.jpg\" style='width: 400px' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5772)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turns out the gain is adjusted for uniform random numbers\n",
    "torch.zeros(100000).uniform_(-1,1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this recreates the original pytorch implementation of kaiming init\n",
    "def kaiming2(x, leaky_amt, use_fan_out=False):\n",
    "    nf, ni, *_ = x.shape\n",
    "    rec_fs = x[0, 0].shape.numel()\n",
    "    \n",
    "    if use_fan_out:\n",
    "        fan = nf * rec_fs  \n",
    "    else:\n",
    "        fan = ni * rec_fs\n",
    "        \n",
    "    std = gain(leaky_amt) / math.sqrt(fan)\n",
    "    bound = math.sqrt(3) * std\n",
    "    x.data.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our custom kaiming function we still get variance = 1, but the mean is still off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4799, grad_fn=<MeanBackward1>),\n",
       " tensor(0.8741, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaiming2(l1.weight, leaky_amt=0)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2216, grad_fn=<MeanBackward1>),\n",
       " tensor(0.3925, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaiming2(l1.weight, leaky_amt=math.sqrt(5.))\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets test out to see what happens w/ default init\n",
    "\n",
    "We are going to make a simple conv2d layer and let the normal pytorch defaults do the initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\" unrolls input tensor to a single shape\"\"\"\n",
    "        return x.view(-1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,8,5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8,16,3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,32,3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32,1,3, stride=2, padding=1),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_valid[:100].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we look at the std, its not zero\n",
    "\n",
    "This is a problem, even when examining the backward grad, the variance is still off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0059, grad_fn=<MeanBackward1>),\n",
       " tensor(0.0115, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_x = model(x)\n",
    "stats(processed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0204), tensor(0.0263))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = mse(processed_x, y)\n",
    "l.backward()\n",
    "stats(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the model again, but with kaiming uniform init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,8,5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8,16,3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16,32,3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32,1,3, stride=2, padding=1),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Flatten(),\n",
    ")\n",
    "\n",
    "for l in model:\n",
    "    if isinstance(l, nn.Conv2d):\n",
    "        init.kaiming_uniform_(l.weight)\n",
    "        l.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2259, grad_fn=<MeanBackward1>),\n",
       " tensor(0.2259, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_x = model(x)\n",
    "stats(processed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0650), tensor(0.3066))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = mse(processed_x, y)\n",
    "l.backward()\n",
    "stats(model[0].weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
